{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "-1 L = [[32.07273388]]\n",
      " 0 L = [[32.07273388]]\n"
     ]
    }
   ],
   "source": [
    "# %load prob1_fit.py\n",
    "import os  \n",
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "IST 597: Foundations of Deep Learning\n",
    "Problem 1: Univariate Regression\n",
    "umar\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "'''\n",
    "\n",
    "# NOTE: you will need to tinker with the meta-parameters below yourself (do not think of them as defaults by any means)\n",
    "# NOTE: You can work in pure eager mode or Keras or even hybrid\n",
    "# NOTE : Use tf.Variable when using gradientTape, if you build your own gradientTape then use simple linear algebra using numpy or tensorflow math\n",
    "\n",
    "# meta-parameters for program\n",
    "alpha = 0.0 # step size coefficient\n",
    "eps = 0.00000 # controls convergence criterion\n",
    "n_epoch = 1 # number of epochs (full passes through the dataset)\n",
    "\n",
    "# begin simulation\n",
    "# Tip0: Use tf.function --> helps in speeding up the training\n",
    "# Example\n",
    "\n",
    "#@tf.function\n",
    "#def regress(X, theta):\n",
    "#\t# WRITEME: write your code here to complete the routine\n",
    "#\t# Define your forward pass\n",
    "#\treturn -1.0\n",
    "\n",
    "def regress(X, theta):\n",
    "\t# WRITEME: write your code here to complete the routine\n",
    "\t# Define your forward pass\n",
    "\treturn -1.0\n",
    "\n",
    "def gaussian_log_likelihood(mu, y):\n",
    "\t# WRITEME: write your code here to complete the sub-routine\n",
    "\t# Define loss function\n",
    "\treturn -1.0\n",
    "\t\n",
    "def computeCost(X, y, theta): # loss is now Bernoulli cross-entropy/log likelihood\n",
    "\t# WRITEME: write your code here to complete the routine\n",
    "        temp = 0\n",
    "        for i in range(0,len(y)):\n",
    "\t        temp = temp + (theta[0] + theta[1]*X[i] - y[i])**2\n",
    "        result = (1/(2*len(y)))*temp\n",
    "        #print(result)\n",
    "\t# Cost function is also known as loss function \n",
    "        return result\n",
    "\t\n",
    "def computeGrad(X, y, theta): \n",
    "\t# WRITEME: write your code here to complete the routine\n",
    "\t# NOTE: you do not have to use the partial derivative symbols below, they are there to guide your thinking)\n",
    "        dL_dfy = None # derivative w.r.t. to model output units (fy)\n",
    "        temp = 0\n",
    "        for i in range(0,len(y)):\n",
    "            temp = temp + (theta[0] + theta[1]*X[i] -  y[i])\n",
    "        dL_db =  temp/len(y) # derivative w.r.t. model weights w\n",
    "        temp = 0\n",
    "        for i in range(0,len(y)):\n",
    "            temp = temp + (theta[0] + theta[1]*X[i] -  y[i])*X[i]\n",
    "        dL_dw = temp/len(y) # derivative w.r.t model bias b\n",
    "        nabla = (dL_db, dL_dw) # nabla represents the full gradient\n",
    "        # You can also use gradient tape and replace this function\n",
    "        return nabla\n",
    "\n",
    "path = os.getcwd() + '/data/prob1.dat'  \n",
    "data = pd.read_csv(path, header=None, names=['X', 'Y']) \n",
    "# Tip1: Convert .dat into numpy and use tensor flow api to process data\n",
    "# display some information about the dataset itself here\n",
    "# WRITEME: write your code here to print out information/statistics about the data-set \"data\" \n",
    "\n",
    "print(len(data))\n",
    "# WRITEME: write your code here to create a simple scatterplot of the dataset itself and print/save to disk the result\n",
    "\n",
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]  \n",
    "X = data.iloc[:,0:cols-1]  \n",
    "y = data.iloc[:,cols-1:cols] \n",
    "\n",
    "# convert from data frames to numpy matrices\n",
    "X = np.array(X.values)  \n",
    "y = np.array(y.values)\n",
    "\n",
    "#TODO convert np array to tensor objects if working with Keras\n",
    "# convert to numpy arrays and initalize the parameter array theta \n",
    "w = np.zeros((1,X.shape[1]))\n",
    "b = np.array([0])\n",
    "theta = (b, w)\n",
    "### Important please read all comments\n",
    "### or use tf.Variable to define w and b if using Keras and gradient tape\n",
    "L = computeCost(X, y, theta)\n",
    "print(\"-1 L = {0}\".format(L))\n",
    "L_best = L\n",
    "i = 0\n",
    "cost = [] # you can use this list variable to help you create the loss versus epoch plot at the end (if you want)\n",
    "\n",
    "while(i < n_epoch):\n",
    "\tdL_db, dL_dw = computeGrad(X, y, theta)\n",
    "\tb = theta[0]\n",
    "\tw = theta[1]\n",
    "\t# update rules go here...\n",
    "\t# WRITEME: write your code here to perform a step of gradient descent & record anything else desired for later\n",
    "\t\n",
    "\t# (note: don't forget to override the theta variable...)\n",
    "\tL = computeCost(X, y, theta) # track our loss after performing a single step\n",
    "\t# Use function \n",
    "\t\n",
    "\tprint(\" {0} L = {1}\".format(i,L))\n",
    "\ti += 1\n",
    "        #TODO\n",
    "\t# print parameter values found after the search\n",
    "\t#print W\n",
    "#print b\n",
    "#Save everything into saver object in tensorflow\n",
    "#Visualize using tensorboard\n",
    "kludge = 0.25 # helps with printing the plots (you can tweak this value if you like)\n",
    "# visualize the fit against the data\n",
    "\n",
    "# WRITEME: write your code here to save plot to disk (look up documentation/inter-webs for matplotlib)\n",
    "\n",
    "# visualize the loss as a function of passes through the dataset\n",
    "# WRITEME: write your code here create and save a plot of loss versus epoch\n",
    "\n",
    "# plt.show() # convenience command to force plots to pop up on desktop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-7e15938992d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'shape' is not defined"
     ]
    }
   ],
   "source": [
    "shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
